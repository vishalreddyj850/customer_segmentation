---
title: "Customer Segmentation for E-commerce using K-means Clustering"
author: "Vishal Reddy Jakkareddy and PruthviRaj Amgoth"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)



## Introduction  

Customer segmentation is an essential strategy in e-commerce that allows businesses to classify customers based on purchasing behaviors, enabling targeted marketing efforts and customer retention strategies. Traditional segmentation relied on demographic attributes, but with advancements in machine learning, data-driven approaches such as K-Means clustering have become widely adopted. This study leverages K-Means clustering to analyze an online retail dataset and identify distinct customer groups based on their purchase behaviors.

Recent research has highlighted the advantages of clustering algorithms in segmenting customers effectively. K-Means clustering remains a popular choice due to its efficiency and ease of implementation [@paramita2024comparison]. Alternative methods such as K-Medoids and DBSCAN have also been explored, showing advantages in handling noise and varying cluster densities [@wu2022research]. This study applies K-Means clustering using R to segment customers from an online retail dataset, enabling a data-driven approach to customer analysis and decision-making.

### Literature Review  

#### K-Means Clustering in E-Commerce  

The effectiveness of K-Means clustering for customer segmentation has been demonstrated in various studies. Tabianan et al. [@tabianan2022k] implemented the K-Means algorithm on a Malaysian e-commerce dataset, improving clustering accuracy by incorporating SAPK. Similarly, Rajput & Singh [@10048834] analyzed session length, time spent on mobile and web platforms, and yearly spending to determine optimal customer segmentation using the elbow method.


#### Optimizing K-Means Clustering  

Bradley et al. [@bradley2000constrained] introduced a constrained K-Means algorithm, which ensures a minimum number of points per cluster, improving the stability of segmentation. Kanungo et al. [@10.1145/336154.336189] proposed a filtering algorithm that optimizes K-Means clustering by using kd-trees, enhancing efficiency in large datasets.

Devaghi & Sudha [@10183143] emphasized the significance of exploratory data analysis (EDA) combined with K-Means clustering for customer segmentation, highlighting the need for robust data preprocessing and feature selection.

#### Real-World Applications  

Several studies illustrate the practical applications of K-Means clustering in e-commerce and retail. Agrawal et al. [@10105070] used a hybrid approach integrating the elbow method and K-Means clustering to analyze e-commerce data, demonstrating its effectiveness in identifying customer segments for personalized marketing. Arul et al. [@9725630] applied K-Means to mall customer data, providing insights that enhanced targeted advertising strategies.


## Methods  

### K-Means Clustering Algorithm  

The **K-Means clustering algorithm** is an iterative approach for partitioning a dataset into `k` clusters. It follows these steps to achieve convergence:

1. **Initialize Cluster Centers:**  
   - Select `k` initial cluster centroids randomly or use **K-Means++** (K-Means++ is an improved initialization technique that places centroids far apart to enhance convergence and accuracy.) for optimized placement.  
   - This ensures centroids are well-separated initially.

2. **Assign Data Points:**  
   - Compute the **distance** between each data point and all centroids.  
   - Assign each data point to the nearest cluster based on a **proximity measure** (e.g., Euclidean distance).  
   - The assignment is done by minimizing the distance:

$$
D(x_i, Z_k(I)) = \min D(x_i, Z_j(I)), j = 1, 2, ..., k
$$

3. **Update Cluster Centers:**  
   - Recalculate the centroid of each cluster by computing the **mean** of all assigned points:

$$
Z_j(I) = \frac{1}{n_j} \sum_{k=1}^{n_j} X_k^{(j)}
$$

Where:

- $X_k^{(j)}$ are the data points assigned to cluster `j`.  
- $Z_j(I)$ are the centroids of clusters.  

4. **Repeat Until Convergence:**  
   - Steps 2 and 3 are repeated iteratively until **no significant change** occurs in the centroids.  
   - The algorithm stops when cluster assignments remain stable or meet a convergence threshold [@tabianan2022k].  
   - The final clustering result aims to minimize the **error square sum criterion**:

$$
J = \sum_{j=1}^{k} \sum_{k=1}^{n_j} ||X_k^{(j)} - Z_j(I)||^2
$$

Where `J` represents the total clustering variance.

![Flowchart of K-Means Clustering Algorithm](Flowchart-of-k-means-clustering-algorithm.png)

*Figure: Flowchart of the K-Means Clustering Algorithm. Source: [@article]*

### Selection of `k` (Number of Clusters)  

The optimal number of clusters `k` is determined using the **Elbow Method**, which plots WCSS against `k` values. The elbow point—where the rate of decrease in WCSS (Within-Cluster Sum of Squares (WCSS) is a measure of the variance within clusters. The elbow point—where the curve bends—indicates the optimal number of clusters.) diminishes—indicates the optimal `k`.


### Proximity Measures  

K-Means clustering assigns points to clusters based on **distance metrics**. The most commonly used metrics are:

- **Euclidean Distance (Most Common in K-Means):**

$$
D_{euclidean}(x_1, x_2) = \sqrt{ \sum_{i=1}^{n} ((x_{1})_{i} - (x_{2})_{i})^2 }
$$

- **Manhattan Distance (Useful for Grid-Based Data):**

$$
D_{manhattan}(x_1, x_2) = \sum_{i=1}^{n} |((x_{1})_{i} - (x_{2})_{i})|
$$

Choosing an appropriate distance metric affects **cluster shape** and performance [@bradley2000constrained].


<!--

## Methods  

### K-Means Clustering Algorithm  

The K-Means clustering algorithm is an iterative approach used to group similar data points into clusters. The algorithm follows these steps:

1. *Initialize Cluster Centers:* Randomly select `k` initial cluster centroids.
2. *Assign Data Points:* Compute the Euclidean distance between each data point and the cluster centroids, assigning each point to the nearest cluster.
3. *Update Cluster Centers:* Recalculate cluster centroids by computing the mean of all data points assigned to each cluster.
4. *Iterate Until Convergence:* Repeat steps 2 and 3 until the cluster assignments remain unchanged or meet a predefined convergence threshold.

This iterative process ensures that clusters are refined until an optimal grouping is achieved [@tabianan2022k].

### Proximity Measures  

K-Means clustering relies on proximity measures to determine cluster assignments. The most common measures include:

- **Euclidean Distance:**

$D_{euclidean}(x_1, x_2) = \sqrt{ \sum_{i=1}^{n} ((x_{1})_{i} - (x_{2})_{i})^2 }$


- **Manhattan Distance:**

$D_{manhattan}(x_1, x_2) = \sum_{i=1}^{n} |((x_{1})_{i} - (x_{2})_{i})|$


These measures influence how clusters are formed and the overall effectiveness of segmentation [@bradley2000constrained].

-->

## Analysis and Results  

### Dataset Description

The Online Retail dataset, sourced from the UCI Machine Learning Repository, contains transactional data from a UK-based e-commerce store. This store specializes in selling unique giftware, which is frequently purchased in bulk by customers. The dataset spans transactions recorded between December 1, 2009, and December 9, 2011. [@online_retail_352]

This dataset is particularly useful for customer segmentation, sales analysis, and market trend evaluation. It includes eight attributes that provide insights into customer purchases, product details, and order quantities. The data can be leveraged for analyzing buying behaviors, identifying customer clusters, and predicting future sales trends.


### Dataset Overview  
The dataset contains transactional records from an online retail store. The key attributes in the dataset include:

- `InvoiceNo`: Unique invoice number for transactions
- `StockCode`: Product code
- `Description`: Product name
- `Quantity`: Quantity purchased
- `InvoiceDate`: Date and time of the purchase
- `UnitPrice`: Price per unit
- `CustomerID`: Unique identifier for customers
- `Country`: Country where the transaction occurred

### Data Preprocessing  

Before clustering, the dataset requires cleaning and transformation. The preprocessing steps include handling missing values, removing duplicates, and transforming categorical data where necessary.

```{r, warning=FALSE, echo=T, message=FALSE}
# Data Manipulation and Cleaning
library(tidyverse)
library(readxl)

# Visualization
library(ggplot2)
library(patchwork)
library(gridExtra)

# Data Exploration and Analysis
library(DT)
library(knitr)
library(gtsummary)
library(naniar)
library(VIM)
library(skimr)

# Clustering Analysis
library(factoextra)
library(cluster)
library(clValid)
library(NbClust)

# Load Dataset
file_path <- "~/customer_segmentation_group_project/Online_Retail.xlsx"
df <- read_excel(file_path)

# Display sample rows
kable(head(df), format = "html", caption = "First Few Records of the Dataset", table.attr = "class='table table-striped table-bordered'")

```


```{r, warning=FALSE, echo=T, message=FALSE}
# Summary Statistics
skim(df)

# Visualizing Missing Values
aggr(df, col = c("navyblue", "red"), numbers = TRUE, sortVars = FALSE,
     cex.axis = 0.8, cex.lab = 1.2, cex.numbers = 1.2)

```

### Data Cleaning and Transformation

```{r}

# Data Cleaning
df <- df %>%
  filter(!is.na(CustomerID)) %>%
  filter(!grepl("^C", InvoiceNo)) %>%
  filter(Quantity > 0, UnitPrice > 0) %>%
  mutate(TotalSpending = Quantity * UnitPrice) %>%
  mutate(InvoiceDate = as.Date(InvoiceDate)) %>%
  mutate(Description = replace_na(Description, "Unknown"))

```

### Exploratory Data Analysis  


<!--1. **Top 10 Most Purchased Products**  
2. **Distribution of Unit Prices (Log Scale)**  
3. **Distribution of Customer Spending (Log Scale)**  
4. **Top 10 Countries by Number of Transactions**  
-->


```{r}
# Quantity Histogram
quantity_plot <- df %>%
  mutate(Quantity_Bins = cut(Quantity, breaks = c(-Inf, 1, 5, 10, 20, 30, 50, Inf))) %>%
  count(Quantity_Bins) %>%
  ggplot(aes(x = Quantity_Bins, y = n, fill = n)) +
  geom_bar(stat = "identity") +
  scale_fill_gradient(low = "blue", high = "darkblue") +
  labs(title = "A: Quantity Histogram", x = "Quantity", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Price Histogram
price_plot <- df %>%
  mutate(Price_Bins = cut(UnitPrice, breaks = c(-Inf, 1, 2, 4, 6, 8, 10, 50, Inf))) %>%
  count(Price_Bins) %>%
  ggplot(aes(x = Price_Bins, y = n, fill = n)) +
  geom_bar(stat = "identity") +
  scale_fill_gradient(low = "blue", high = "darkblue") +
  labs(title = "B: Price Histogram", x = "Price", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Top 5 StockCode
top_stockcode <- df %>%
  count(StockCode) %>%
  arrange(desc(n)) %>%
  head(5) %>%
  ggplot(aes(x = reorder(StockCode, -n), y = n, fill = StockCode)) +
  geom_bar(stat = "identity") +
  labs(title = "C: Top 5 StockCode", x = "StockCode", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Top 5 Countries
top_countries <- df %>%
  count(Country) %>%
  arrange(desc(n)) %>%
  head(5) %>%
  ggplot(aes(x = reorder(Country, -n), y = n, fill = Country)) +
  geom_bar(stat = "identity") +
  labs(title = "D: Top 5 Country", x = "Country", y = "Count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Combine Plots
(quantity_plot | price_plot) /
(top_stockcode | top_countries)


```

### Outlier Detection and Handling


```{r}
# Outlier Removal Function
remove_outliers <- function(data, column) {
  Q1 <- quantile(data[[column]], 0.25)
  Q3 <- quantile(data[[column]], 0.75)
  IQR_value <- Q3 - Q1
  lower_bound <- Q1 - 1.5 * IQR_value
  upper_bound <- Q3 + 1.5 * IQR_value
  data %>% filter(data[[column]] >= lower_bound & data[[column]] <= upper_bound)
}

# Removing Outliers
df <- df %>%
  remove_outliers("Quantity") %>%
  remove_outliers("UnitPrice")

# RFM Analysis Data Preparation
customer_data <- df %>%
  group_by(CustomerID) %>%
  summarise(
    Recency = as.numeric(as.Date("2011-12-10") - max(InvoiceDate)),
    Frequency = n(),
    Monetary = sum(TotalSpending)
  )

# Log Transformation
customer_data_log <- customer_data %>%
  mutate(
    Frequency = log1p(Frequency),
    Monetary = log1p(Monetary)
  )

# Standardizing Data for K-Means Clustering
customer_data_scaled <- customer_data_log %>%
  select(-CustomerID) %>%
  scale()

# Visualizing Transformed Data
p1 <- ggplot(customer_data_log, aes(y = Recency)) + geom_boxplot(fill = "#4CAF50")
p2 <- ggplot(customer_data_log, aes(y = Frequency)) + geom_boxplot(fill = "#FFC107")
p3 <- ggplot(customer_data_log, aes(y = Monetary)) + geom_boxplot(fill = "#2196F3")

grid.arrange(p1, p2, p3, ncol = 3)

```



### K-Means Clustering Analysis  

#### Finding Optimal k for Clustering

###### Elbow Method

```{r}
set.seed(42)
fviz_nbclust(customer_data_scaled, kmeans, method = "wss") +
    geom_vline(xintercept = 3, linetype = 2)+
  labs(subtitle = "Elbow Method")

```


<!--
###### Silhouette Method

```{r}
set.seed(42)
fviz_nbclust(customer_data_scaled, kmeans, method = "silhouette") +
  labs(title = "Silhouette Method for Optimal k")
```

###### Gap Statistic

```{r}
set.seed(42)
gap_stat <- clusGap(customer_data_scaled, nstart = 25, kmeans, K.max = 10, B = 50)
fviz_gap_stat(gap_stat) +
  labs(title = "Gap Statistic for Optimal k")
```
-->

#### Performing K-Means Clustering

```{r}
# K-Means Clustering
# Optimal k (based on visual inspection — best is k = 3)
set.seed(42)
k <- 3
kmeans_model <- kmeans(customer_data_scaled, centers = k, nstart = 25)

# Adding Cluster Labels
customer_data_log$Cluster <- as.factor(kmeans_model$cluster)

# Visualizing Clusters
fviz_cluster(kmeans_model, data = customer_data_scaled) +
  labs(title = "Customer Segmentation using K-Means Clustering")

```

### Cluster Insights
```{r}
# Cluster Profiling
cluster_summary <- customer_data_log %>%
  group_by(Cluster) %>%
  summarise(
    Avg_Recency = mean(Recency),
    Avg_Frequency = mean(Frequency),
    Avg_Monetary = mean(Monetary),
    Total_Customers = n()
  )

# Visualizing Cluster Insights
p1 <- ggplot(cluster_summary, aes(x = Cluster, y = Avg_Recency)) +
  geom_bar(stat = "identity", fill = "#4CAF50") +
  labs(title = "Average Recency by Cluster")

p2 <- ggplot(cluster_summary, aes(x = Cluster, y = Avg_Frequency)) +
  geom_bar(stat = "identity", fill = "#FFC107") +
  labs(title = "Average Frequency by Cluster")

p3 <- ggplot(cluster_summary, aes(x = Cluster, y = Avg_Monetary)) +
  geom_bar(stat = "identity", fill = "#2196F3") +
  labs(title = "Average Monetary Value by Cluster")

grid.arrange(p1, p2, p3, ncol = 3)

# Final Cluster Summary Output
kable(cluster_summary, format = "html", caption = "First Few Records of the Dataset", table.attr = "class='table table-striped table-bordered'")

```

### Results 


### Conclusion  
The K-Means clustering algorithm successfully identified distinct customer segments, providing actionable insights for targeted marketing strategies.


## References  

```{r references, echo=FALSE}
# Include references from the bibliography file
```