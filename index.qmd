---
title: "Customer Segmentation for E-commerce using K-means Clustering"
author: "Vishal Reddy Jakkareddy and PruthviRaj Amgoth"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)



## Introduction  

Customer segmentation is an essential strategy in e-commerce that allows businesses to classify customers based on purchasing behaviors, enabling targeted marketing efforts and customer retention strategies. Traditional segmentation relied on demographic attributes, but with advancements in machine learning, data-driven approaches such as K-Means clustering have become widely adopted. This study leverages K-Means clustering to analyze an online retail dataset and identify distinct customer groups based on their purchase behaviors.

Recent research has highlighted the advantages of clustering algorithms in segmenting customers effectively. K-Means clustering remains a popular choice due to its efficiency and ease of implementation [@paramita2024comparison]. Alternative methods such as K-Medoids and DBSCAN have also been explored, showing advantages in handling noise and varying cluster densities [@wu2022research]. This study applies K-Means clustering using R to segment customers from an online retail dataset, enabling a data-driven approach to customer analysis and decision-making.

### Literature Review  

#### K-Means Clustering in E-Commerce  

The effectiveness of K-Means clustering for customer segmentation has been demonstrated in various studies. Tabianan et al. [@tabianan2022k] implemented the K-Means algorithm on a Malaysian e-commerce dataset, improving clustering accuracy by incorporating SAPK. Similarly, Rajput & Singh [@10048834] analyzed session length, time spent on mobile and web platforms, and yearly spending to determine optimal customer segmentation using the elbow method.


#### Optimizing K-Means Clustering  

Bradley et al. [@bradley2000constrained] introduced a constrained K-Means algorithm, which ensures a minimum number of points per cluster, improving the stability of segmentation. Kanungo et al. [@10.1145/336154.336189] proposed a filtering algorithm that optimizes K-Means clustering by using kd-trees, enhancing efficiency in large datasets.

Devaghi & Sudha [@10183143] emphasized the significance of exploratory data analysis (EDA) combined with K-Means clustering for customer segmentation, highlighting the need for robust data preprocessing and feature selection.

#### Real-World Applications  

Several studies illustrate the practical applications of K-Means clustering in e-commerce and retail. Agrawal et al. [@10105070] used a hybrid approach integrating the elbow method and K-Means clustering to analyze e-commerce data, demonstrating its effectiveness in identifying customer segments for personalized marketing. Arul et al. [@9725630] applied K-Means to mall customer data, providing insights that enhanced targeted advertising strategies.


## Methods  

#### K-Means Clustering Algorithm  

The **K-Means clustering algorithm** is an iterative approach for partitioning a dataset into `k` clusters. It follows these steps to achieve convergence:

1. **Initialize Cluster Centers:**
   - Select `k` initial cluster centroids randomly or use **K-Means++** for optimized placement.
   - This ensures centroids are well-separated initially.

2. **Assign Data Points:**
   - Compute the **distance** between each data point and all centroids.
   - Assign each data point to the nearest cluster based on a **proximity measure** (e.g., Euclidean distance).

3. **Update Cluster Centers:**
   - Recalculate the centroid of each cluster by computing the **mean** of all assigned points.

4. **Repeat Until Convergence:**
   - Steps 2 and 3 are repeated iteratively until **no significant change** occurs in the centroids.
   - The algorithm stops when cluster assignments remain stable or meet a convergence threshold [@tabianan2022k].

#### Selection of `k` (Number of Clusters)  

The optimal number of clusters `k` is determined using the **Elbow Method**, which plots WCSS against `k` values. The elbow point—where the rate of decrease in WCSS diminishes—indicates the optimal `k`.


#### Proximity Measures  

K-Means clustering assigns points to clusters based on **distance metrics**. The most commonly used metrics are:

- **Euclidean Distance (Most Common in K-Means)**:

$$
D_{euclidean}(x_1, x_2) = \sqrt{ \sum_{i=1}^{n} ((x_{1})_{i} - (x_{2})_{i})^2 }
$$

- **Manhattan Distance (Useful for Grid-Based Data)**:

$$
D_{manhattan}(x_1, x_2) = \sum_{i=1}^{n} |((x_{1})_{i} - (x_{2})_{i})|
$$

Choosing an appropriate distance metric affects **cluster shape** and performance [@bradley2000constrained].

<!--

#### Algorithm Workflow  

The process of clustering follows the iterative steps of initialization, assignment, and updating cluster centroids. The figure below illustrates how data points are assigned to cluster centers and updated iteratively.

<center>![K-Means Clustering Process (Image Source: @bradley2000constrained)](image.png)</center>

This image provides a conceptual representation of data points, centroids, and the clustering assignment step. It visualizes how centroids adjust iteratively to achieve convergence. 
-->

#### Mathematical Formulation  

Given a dataset `D` with `n` data points and `k` clusters, the clustering process follows [@deng2020retracted]:

1. Define initial centroids $( Z_j(I), j = 1, 2, ..., k )$
2. Compute the distance $( D(x_i, Z_j(I)) )$ between each data point $( x_i )$ and cluster center $( Z_j )$.
3. Assign $( x_i )$ to the cluster with the nearest centroid:

$$
D(x_i, Z_k(I)) = \min D(x_i, Z_j(I)), j = 1, 2, ..., k
$$

4. Recalculate new cluster centers by minimizing the error square sum criterion:

$$
J = \sum_{j=1}^{k} \sum_{k=1}^{n_j} ||X_k^{(j)} - Z_j(I)||^2
$$

Where:

- $( J )$ represents the total clustering variance.

- $( X_k^{(j)} )$ are the data points assigned to cluster $( j )$.

- $( Z_j(I) )$ are the centroids of clusters.

This iterative process continues until convergence is reached, ensuring clusters are well-separated and compact.

<!--

## Methods  

### K-Means Clustering Algorithm  

The K-Means clustering algorithm is an iterative approach used to group similar data points into clusters. The algorithm follows these steps:

1. *Initialize Cluster Centers:* Randomly select `k` initial cluster centroids.
2. *Assign Data Points:* Compute the Euclidean distance between each data point and the cluster centroids, assigning each point to the nearest cluster.
3. *Update Cluster Centers:* Recalculate cluster centroids by computing the mean of all data points assigned to each cluster.
4. *Iterate Until Convergence:* Repeat steps 2 and 3 until the cluster assignments remain unchanged or meet a predefined convergence threshold.

This iterative process ensures that clusters are refined until an optimal grouping is achieved [@tabianan2022k].

### Proximity Measures  

K-Means clustering relies on proximity measures to determine cluster assignments. The most common measures include:

- **Euclidean Distance:**

$D_{euclidean}(x_1, x_2) = \sqrt{ \sum_{i=1}^{n} ((x_{1})_{i} - (x_{2})_{i})^2 }$


- **Manhattan Distance:**

$D_{manhattan}(x_1, x_2) = \sum_{i=1}^{n} |((x_{1})_{i} - (x_{2})_{i})|$


These measures influence how clusters are formed and the overall effectiveness of segmentation [@bradley2000constrained].

-->

## Analysis and Results  

### Dataset Description

The Online Retail dataset, sourced from the UCI Machine Learning Repository, contains transactional data from a UK-based e-commerce store. This store specializes in selling unique giftware, which is frequently purchased in bulk by customers. The dataset spans transactions recorded between December 1, 2009, and December 9, 2011. [@online_retail_352]

This dataset is particularly useful for customer segmentation, sales analysis, and market trend evaluation. It includes eight attributes that provide insights into customer purchases, product details, and order quantities. The data can be leveraged for analyzing buying behaviors, identifying customer clusters, and predicting future sales trends.


### Dataset Overview  
The dataset contains transactional records from an online retail store. The key attributes in the dataset include:

- `InvoiceNo`: Unique invoice number for transactions
- `StockCode`: Product code
- `Description`: Product name
- `Quantity`: Quantity purchased
- `InvoiceDate`: Date and time of the purchase
- `UnitPrice`: Price per unit
- `CustomerID`: Unique identifier for customers
- `Country`: Country where the transaction occurred

### Data Preprocessing  

Before clustering, the dataset requires cleaning and transformation. The preprocessing steps include handling missing values, removing duplicates, and transforming categorical data where necessary.

```{r, warning=FALSE, echo=T, message=FALSE}
library(tidyverse)
library(readxl)
library(DT)
library(knitr)
library(gtsummary)
library(naniar)
library(VIM)
library(patchwork)
library(ggplot2)
library(skimr)
library(factoextra)


file_path <- "~/customer_segmentation_group_project/Online_Retail.xlsx"

df <- read_excel(file_path)


kable(as.data.frame(head(df)), format = "html", caption = "1st few values of dataset")

```


```{r, warning=FALSE, echo=T, message=FALSE}

skim(df)

# Aggregated missing values visualization
aggr(df, col = c("navyblue", "red"), numbers = TRUE, sortVars = TRUE,
     cex.axis = 0.8, cex.lab = 1.2, cex.numbers = 1.2)



```


### Exploratory Data Analysis  

1. **Top 10 Most Purchased Products**  
2. **Distribution of Unit Prices (Log Scale)**  
3. **Distribution of Customer Spending (Log Scale)**  
4. **Top 10 Countries by Number of Transactions**  


```{r}


# Data Cleaning
df <- df %>%
  drop_na(CustomerID) %>%
  filter(!grepl("^C", InvoiceNo)) %>%
  filter(Quantity > 0, UnitPrice > 0) %>%
  mutate(TotalSpending = Quantity * UnitPrice)



# Top 10 most purchased products
top_products <- df %>%
  group_by(Description) %>%
  summarise(TotalQuantity = sum(Quantity)) %>%
  arrange(desc(TotalQuantity)) %>%
  head(10)

ggplot(top_products, aes(x = reorder(Description, TotalQuantity), y = TotalQuantity)) +
  geom_bar(stat = "identity", fill = "gray") +
  coord_flip() +
  labs(title = "Top 10 Most Purchased Products", x = "Product", y = "Total Quantity")

# Distribution of Unit Prices
ggplot(df, aes(x = UnitPrice)) +
  geom_histogram(bins = 50, fill = "blue", color = "black") +
  scale_x_log10() +
  labs(title = "Distribution of Unit Price (Log Scale)", x = "Unit Price", y = "Count")

# Distribution of Customer Spending
customer_spending <- df %>%
  group_by(CustomerID) %>%
  summarise(TotalSpending = sum(TotalSpending))

ggplot(customer_spending, aes(x = TotalSpending)) +
  geom_histogram(bins = 50, fill = "purple", color = "black") +
  scale_x_log10() +
  labs(title = "Distribution of Customer Spending (Log Scale)", x = "Total Spending", y = "Number of Customers")
```

### K-Means Clustering Analysis  

#### Selecting the Optimal `k` using the Elbow Method
```{r}
# Customer Aggregation
customer_data <- df %>%
  group_by(CustomerID) %>%
  summarise(
    Recency = as.numeric(as.Date("2011-12-10") - max(as.Date(InvoiceDate))),
    Frequency = n(),
    Monetary = sum(TotalSpending)
  )

# Normalizing Data
customer_data_scaled <- scale(customer_data[, -1])

# Elbow Method
set.seed(42)
fviz_nbclust(customer_data_scaled, kmeans, method = "wss") +
  labs(title = "Elbow Method for Optimal k")
```

#### Performing K-Means Clustering
```{r}
# K-Means Clustering
set.seed(42)
k <- 4
kmeans_model <- kmeans(customer_data_scaled, centers = k, nstart = 25)
customer_data$Cluster <- as.factor(kmeans_model$cluster)

# Visualizing Clusters
fviz_cluster(kmeans_model, data = customer_data_scaled) +
  labs(title = "Customer Segmentation using K-Means Clustering")
```

### Cluster Insights
```{r}
# Cluster Summary
cluster_summary <- customer_data %>%
  group_by(Cluster) %>%
  summarise(
    Avg_Recency = mean(Recency),
    Avg_Frequency = mean(Frequency),
    Avg_Monetary = mean(Monetary),
    Total_Customers = n()
  )

# Visualizing Cluster Insights
p1 <- ggplot(cluster_summary, aes(x = Cluster, y = Avg_Recency)) +
  geom_bar(stat = "identity", fill = "#4CAF50") +
  labs(title = "Average Recency by Cluster")

p2 <- ggplot(cluster_summary, aes(x = Cluster, y = Avg_Frequency)) +
  geom_bar(stat = "identity", fill = "#FFC107") +
  labs(title = "Average Frequency by Cluster")

p3 <- ggplot(cluster_summary, aes(x = Cluster, y = Avg_Monetary)) +
  geom_bar(stat = "identity", fill = "#2196F3") +
  labs(title = "Average Monetary Value by Cluster")

(p1 | p2 | p3)
```

## Results and Discussion  

The analysis reveals clear customer segments with distinct behavior patterns. The key observations include:

- **Cluster 3** represents high-value customers with frequent purchases and significant monetary value.
- **Cluster 1** includes inactive customers with minimal recent purchases.
- **Cluster 4** exhibits moderate spending behavior, suggesting opportunities for increased engagement.
- **Cluster 2** reflects regular buyers with consistent purchase behavior.

### Conclusion  
The K-Means clustering algorithm successfully identified distinct customer segments, providing actionable insights for targeted marketing strategies.


## References  

```{r references, echo=FALSE}
# Include references from the bibliography file
```